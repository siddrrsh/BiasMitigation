{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import random\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from vocab import Vocab\n",
    "from featurize import formatBatchedExamples, constructEmbeddingTensorFromVocabAndWvs, getExampleSubset\n",
    "from models.POSTagger import POSTagger\n",
    "from modelUtil import train_step, test, precisionRecallEval\n",
    "\n",
    "from DataLoader import loadNERDatasetXY\n",
    "from DataBatch import SeqDataBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data parameters (The whole pipeline will need to rerun if these are changed)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "TRAIN_PERCENT = 0.8\n",
    "VAL_PERCENT = 0.1\n",
    "TEST_PERCENT = 0.1\n",
    "\n",
    "DATA_TARGET_POS = 1\n",
    "DATA_TARGET_CHUNK = 2\n",
    "DATA_TARGET_NER = 3\n",
    "\n",
    "#Learning Parameters (Only the model training code will need to rerun if these are changed)\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 25\n",
    "USE_CUDA = False\n",
    "DEBUG_INTERVAL = 250\n",
    "L2_REG = 0.001\n",
    "MOMENTUM = 0.25\n",
    "\n",
    "DEBIAS_EPS = 1e-10\n",
    "\n",
    "BIASED_WVS = \"data/wvs/data_gender_attributes_optm_json_role_biasedEmbeddingsOut.w2v\"\n",
    "DEBIASED_WVS = \"data/wvs/data_gender_attributes_optm_json_role_hardDebiasedEmbeddingsOut.w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CONLL2003 NER Data\n",
      "Loading biased word vectors\n",
      "Loading debiased word vectors\n",
      "Handling wv Edge Cases\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CONLL2003 NER Data\")\n",
    "trainX, trainY = loadNERDatasetXY(\"./data/ner/train.txt\", sourceField=0, targetField=DATA_TARGET_NER, sortLength=True)\n",
    "testX,  testY  = loadNERDatasetXY(\"./data/ner/test.txt\", sourceField=0, targetField=DATA_TARGET_NER, sortLength=True)\n",
    "valX,   valY   = loadNERDatasetXY(\"./data/ner/valid.txt\", sourceField=0, targetField=DATA_TARGET_NER, sortLength=True)\n",
    "\n",
    "print(\"Loading biased word vectors\")\n",
    "biasedWvs = {}\n",
    "f = open(BIASED_WVS)\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for line in lines[1:]:\n",
    "    wv = line.split(\" \")\n",
    "    biasedWvs[wv[0].lower()] = [float(v) for v in wv[1:]]\n",
    "    \n",
    "print(\"Loading debiased word vectors\")\n",
    "debiasedWvs = {}\n",
    "f = open(DEBIASED_WVS)\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for line in lines[1:]:\n",
    "    wv = line.split(\" \")\n",
    "    debiasedWvs[wv[0].lower()] = [float(v) for v in wv[1:]]\n",
    "    EMBEDDING_SIZE = len([float(v) for v in wv[1:]])\n",
    "    \n",
    "print(\"Handling wv Edge Cases\")\n",
    "unk_wv = [float(v) for v in np.random.rand(EMBEDDING_SIZE)]\n",
    "pad_wv = [float(v) for v in np.random.rand(EMBEDDING_SIZE)]\n",
    "\n",
    "biasedWvs[UNK_TOKEN.lower()] = unk_wv\n",
    "biasedWvs[PAD_TOKEN.lower()] = pad_wv\n",
    "debiasedWvs[UNK_TOKEN.lower()] = unk_wv\n",
    "debiasedWvs[PAD_TOKEN.lower()] = pad_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting words that had their baises altered\n",
      "Constructing test set containing examples with debiased terms.\n"
     ]
    }
   ],
   "source": [
    "print(\"Detecting words that had their baises altered\")\n",
    "debiasedTerms = []\n",
    "for key in biasedWvs.keys():\n",
    "    totalDiff = sum([math.fabs(a-b) for a, b in zip(biasedWvs[key], debiasedWvs[key])])\n",
    "    if(totalDiff > DEBIAS_EPS):\n",
    "        debiasedTerms.append(key)\n",
    "\n",
    "print(\"Constructing test set containing examples with debiased terms.\")\n",
    "testX_bias, testY_bias = getExampleSubset(testX, testY, debiasedTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing input Vocabulary\n",
      "Constructed an input vocabulary with 302721 entries\n",
      "Constructing output vocabulary\n",
      "Constructed an output vocabulary with 11 entries\n"
     ]
    }
   ],
   "source": [
    "print(\"Constructing input Vocabulary\")\n",
    "inputVocab = Vocab(UNK_TOKEN.lower())\n",
    "for word in biasedWvs.keys():\n",
    "    inputVocab.add(word.lower())\n",
    "inputVocab.add(PAD_TOKEN.lower())\n",
    "print(\"Constructed an input vocabulary with\", len(inputVocab), \"entries\")\n",
    "\n",
    "print(\"Constructing output vocabulary\")\n",
    "outputVocab = Vocab(UNK_TOKEN.lower())\n",
    "for ty in trainY:\n",
    "    for label in ty:\n",
    "        outputVocab.add(label.lower())\n",
    "outputVocab.add(PAD_TOKEN.lower())\n",
    "print(\"Constructed an output vocabulary with\", len(outputVocab), \"entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching Training Data\n",
      "Generated 235 Train Batches\n",
      "Generated 55 Val Batches\n",
      "Generated 58 Test Batches\n",
      "Generated 473 Test Bias Batches\n"
     ]
    }
   ],
   "source": [
    "print(\"Batching Training Data\")\n",
    "\n",
    "batchedTrainData = []\n",
    "for i, (x, y) in enumerate(zip(trainX, trainY)):\n",
    "    if(i % BATCH_SIZE == 0):\n",
    "        batchedTrainData.append(SeqDataBatch([], [], inputVocab, outputVocab))\n",
    "    batchedTrainData[-1].addXY(x, y)\n",
    "\n",
    "formattedBatchedTrainData = []\n",
    "for b in batchedTrainData:\n",
    "    b.padBatch(PAD_TOKEN, padX=True, padY=True, padingType=\"left\")\n",
    "    x, y = b.getNumericXY(\"torch\", unkify=True)\n",
    "    formattedBatchedTrainData.append((x.long(), y.long()))\n",
    "print(\"Generated\", len(formattedBatchedTrainData), \"Train Batches\")\n",
    "\n",
    "batchedValData = []\n",
    "for i, (x, y) in enumerate(zip(valX, valY)):\n",
    "    if(i % BATCH_SIZE == 0):\n",
    "        batchedValData.append(SeqDataBatch([], [], inputVocab, outputVocab))\n",
    "    batchedValData[-1].addXY(x, y)\n",
    "\n",
    "formattedBatchedValData = []\n",
    "for b in batchedValData:\n",
    "    b.padBatch(PAD_TOKEN, padX=True, padY=True, padingType=\"left\")\n",
    "    x, y = b.getNumericXY(\"torch\", unkify=True)\n",
    "    formattedBatchedValData.append((x.long(), y.long()))\n",
    "print(\"Generated\", len(formattedBatchedValData), \"Val Batches\")\n",
    "\n",
    "batchedTestData = []\n",
    "for i, (x, y) in enumerate(zip(testX, testY)):\n",
    "    if(i % BATCH_SIZE == 0):\n",
    "        batchedTestData.append(SeqDataBatch([], [], inputVocab, outputVocab))\n",
    "    batchedTestData[-1].addXY(x, y)\n",
    "\n",
    "formattedBatchedTestData = []\n",
    "for b in batchedTestData:\n",
    "    b.padBatch(PAD_TOKEN, padX=True, padY=True, padingType=\"left\")\n",
    "    x, y = b.getNumericXY(\"torch\", unkify=True)\n",
    "    formattedBatchedTestData.append((x.long(), y.long()))\n",
    "print(\"Generated\", len(formattedBatchedTestData), \"Test Batches\")\n",
    "\n",
    "batchedBiasTestData = []\n",
    "for i, (x, y) in enumerate(zip(testX_bias, testY_bias)):\n",
    "    if(i % BATCH_SIZE == 0):\n",
    "        batchedBiasTestData.append(SeqDataBatch([], [], inputVocab, outputVocab))\n",
    "    batchedBiasTestData[-1].addXY(x, y)\n",
    "\n",
    "formattedBatchedBiasTestData = []\n",
    "for b in batchedBiasTestData:\n",
    "    b.padBatch(PAD_TOKEN, padX=True, padY=True, padingType=\"left\")\n",
    "    x, y = b.getNumericXY(\"torch\", unkify=True)\n",
    "    formattedBatchedBiasTestData.append((x.long(), y.long()))\n",
    "print(\"Generated\", len(formattedBatchedBiasTestData), \"Test Bias Batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training from biased word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "posModel = POSTagger(EMBEDDING_SIZE, 20, len(inputVocab), len(outputVocab))\n",
    "posModel.setEmbeddings(constructEmbeddingTensorFromVocabAndWvs(biasedWvs, inputVocab, EMBEDDING_SIZE), freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch #1 - \n",
      "\tVal Loss: 0.000970 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #2 - \n",
      "\tVal Loss: 0.000531 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #3 - \n",
      "\tVal Loss: 0.000636 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #4 - \n",
      "\tVal Loss: 0.000861 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #5 - \n",
      "\tVal Loss: 0.000934 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #6 - \n",
      "\tVal Loss: 0.000963 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #7 - \n",
      "\tVal Loss: 0.000994 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #8 - \n",
      "\tVal Loss: 0.001026 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #9 - \n",
      "\tVal Loss: 0.001055 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #10 - \n",
      "\tVal Loss: 0.001081 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #11 - \n",
      "\tVal Loss: 0.001104 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #12 - \n",
      "\tVal Loss: 0.001123 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #13 - \n",
      "\tVal Loss: 0.001140 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #14 - \n",
      "\tVal Loss: 0.001155 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #15 - \n",
      "\tVal Loss: 0.001167 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #16 - \n",
      "\tVal Loss: 0.001177 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #17 - \n",
      "\tVal Loss: 0.001186 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #18 - \n",
      "\tVal Loss: 0.001193 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #19 - \n",
      "\tVal Loss: 0.001199 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #20 - \n",
      "\tVal Loss: 0.001204 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #21 - \n",
      "\tVal Loss: 0.001208 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #22 - \n",
      "\tVal Loss: 0.001211 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #23 - \n",
      "\tVal Loss: 0.001214 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #24 - \n",
      "\tVal Loss: 0.001216 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #25 - \n",
      "\tVal Loss: 0.001218 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Found best case validation loss to be 0.0005314104378285238\n",
      "\t... Loading saved model and testing\n",
      "TEST DATA -\n",
      "\tTest Loss: 0.000811 \n",
      "\tTest Precision 1.000000 \n",
      "\tTest Recall 0.999981 \n",
      "\tTest Macro F1 0.999995\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "optimizer = optim.RMSprop(posModel.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=L2_REG)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Starting Training\")\n",
    "\n",
    "start = True\n",
    "bestValLoss = test(posModel, device, formattedBatchedValData, criterion)    \n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train_step(posModel, device, formattedBatchedTrainData, optimizer, criterion, epoch, DEBUG_INTERVAL)\n",
    "    val_loss = test(posModel, device, formattedBatchedValData, criterion)\n",
    "    precision, recall, f1 = precisionRecallEval(posModel, device, formattedBatchedValData)\n",
    "    print(\"Epoch #{} - \\n\\tVal Loss: {:.6f} \\n\\tVal Precision {:6f} \\n\\tVal Recall {:6f} \\n\\tVal Macro F1 {:6f}\".format(epoch, val_loss, precision, recall, f1))\n",
    "    if(val_loss < bestValLoss and not start):\n",
    "        torch.save(posModel.state_dict(), \"models/savedModels/model.m\")\n",
    "        bestValLoss = val_loss\n",
    "    start = False\n",
    "\n",
    "print(\"Found best case validation loss to be \" + str(bestValLoss) + \"\\n\\t... Loading saved model and testing\")\n",
    "posModel.load_state_dict(torch.load(\"models/savedModels/model.m\"))\n",
    "test_loss = test(posModel, device, formattedBatchedTestData, criterion)\n",
    "precision, recall, f1 = precisionRecallEval(posModel, device, formattedBatchedTestData)\n",
    "print(\"TEST DATA -\\n\\tTest Loss: {:.6f} \\n\\tTest Precision {:6f} \\n\\tTest Recall {:6f} \\n\\tTest Macro F1 {:6f}\".format(test_loss, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= BIASED EMBEDDINGS TEST RESULTS =============\n",
      "loss: 0.00017368417236148794\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "============= DEBIASED EMBEDDINGS TEST RESULTS =============\n",
      "loss: 0.0011560477208451038\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "============= EMBEDDINGS COMPARISION RESULTS =============\n",
      "delta loss: 0.0009823635484836157\n",
      "delta precision: 0.0\n",
      "delta recall: 0.0\n",
      "delta f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "posModel.load_state_dict(torch.load(\"models/savedModels/model.m\"))\n",
    "posModel.setEmbeddings(constructEmbeddingTensorFromVocabAndWvs(biasedWvs, inputVocab, EMBEDDING_SIZE), freeze=True)\n",
    "biased_precision, biased_recall, biased_f1 = precisionRecallEval(posModel, device, formattedBatchedBiasTestData)\n",
    "test_biased_loss = test(posModel, device, formattedBatchedBiasTestData, criterion)\n",
    "print(\"============= BIASED EMBEDDINGS TEST RESULTS =============\")\n",
    "print(\"loss: \" + str(test_biased_loss))\n",
    "print(\"precision: \" + str(biased_precision))\n",
    "print(\"recall: \" + str(biased_recall))\n",
    "print(\"f1: \" + str(biased_f1))\n",
    "\n",
    "posModel.load_state_dict(torch.load(\"models/savedModels/model.m\"))\n",
    "posModel.setEmbeddings(constructEmbeddingTensorFromVocabAndWvs(debiasedWvs, inputVocab, EMBEDDING_SIZE), freeze=True)\n",
    "debiased_precision, debiased_recall, debiased_f1 = precisionRecallEval(posModel, device, formattedBatchedBiasTestData)\n",
    "test_debiased_loss = test(posModel, device, formattedBatchedBiasTestData, criterion)\n",
    "print(\"============= DEBIASED EMBEDDINGS TEST RESULTS =============\")\n",
    "print(\"loss: \" + str(test_debiased_loss))\n",
    "print(\"precision: \" + str(debiased_precision))\n",
    "print(\"recall: \" + str(debiased_recall))\n",
    "print(\"f1: \" + str(debiased_f1))\n",
    "\n",
    "print(\"============= EMBEDDINGS COMPARISION RESULTS =============\")\n",
    "print(\"delta loss: \" + str(test_debiased_loss - test_biased_loss))\n",
    "print(\"delta precision: \" + str(debiased_precision - biased_precision))\n",
    "print(\"delta recall: \" + str(debiased_recall - biased_recall))\n",
    "print(\"delta f1: \" + str(debiased_f1 - biased_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training from debiased word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "debias_posModel = POSTagger(EMBEDDING_SIZE, 20, len(inputVocab), len(outputVocab))\n",
    "debias_posModel.setEmbeddings(constructEmbeddingTensorFromVocabAndWvs(debiasedWvs, inputVocab, EMBEDDING_SIZE), freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "Epoch #1 - \n",
      "\tVal Loss: 0.001901 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #2 - \n",
      "\tVal Loss: 0.000793 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #3 - \n",
      "\tVal Loss: 0.000681 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #4 - \n",
      "\tVal Loss: 0.000928 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #5 - \n",
      "\tVal Loss: 0.001075 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #6 - \n",
      "\tVal Loss: 0.001103 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #7 - \n",
      "\tVal Loss: 0.001134 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #8 - \n",
      "\tVal Loss: 0.001173 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #9 - \n",
      "\tVal Loss: 0.001212 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #10 - \n",
      "\tVal Loss: 0.001246 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #11 - \n",
      "\tVal Loss: 0.001277 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #12 - \n",
      "\tVal Loss: 0.001304 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #13 - \n",
      "\tVal Loss: 0.001327 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #14 - \n",
      "\tVal Loss: 0.001346 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #15 - \n",
      "\tVal Loss: 0.001362 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #16 - \n",
      "\tVal Loss: 0.001375 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #17 - \n",
      "\tVal Loss: 0.001385 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #18 - \n",
      "\tVal Loss: 0.001394 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #19 - \n",
      "\tVal Loss: 0.001401 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #20 - \n",
      "\tVal Loss: 0.001407 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #21 - \n",
      "\tVal Loss: 0.001412 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #22 - \n",
      "\tVal Loss: 0.001415 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #23 - \n",
      "\tVal Loss: 0.001418 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #24 - \n",
      "\tVal Loss: 0.001421 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Epoch #25 - \n",
      "\tVal Loss: 0.001423 \n",
      "\tVal Precision 1.000000 \n",
      "\tVal Recall 1.000000 \n",
      "\tVal Macro F1 1.000000\n",
      "Found best case validation loss to be 0.0006805931072155334\n",
      "... Loading saved model and testing\n",
      "TEST DATA -\n",
      "\tTest Loss: 0.000757 \n",
      "\tTest Precision 1.000000 \n",
      "\tTest Recall 1.000000 \n",
      "\tTest Macro F1 1.000000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "optimizer = optim.RMSprop(debias_posModel.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=L2_REG)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Starting Training\")\n",
    "\n",
    "start = True\n",
    "bestValLoss = test(debias_posModel, device, formattedBatchedValData, criterion)   \n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train_step(debias_posModel, device, formattedBatchedTrainData, optimizer, criterion, epoch, DEBUG_INTERVAL)\n",
    "    val_loss = test(debias_posModel, device, formattedBatchedValData, criterion)\n",
    "    precision, recall, f1 = precisionRecallEval(debias_posModel, device, formattedBatchedValData)\n",
    "    print(\"Epoch #{} - \\n\\tVal Loss: {:.6f} \\n\\tVal Precision {:6f} \\n\\tVal Recall {:6f} \\n\\tVal Macro F1 {:6f}\".format(epoch, val_loss, precision, recall, f1))\n",
    "    if(val_loss < bestValLoss and not start):\n",
    "        torch.save(debias_posModel.state_dict(), \"models/savedModels/debiased_model.m\")\n",
    "        bestValLoss = val_loss\n",
    "    start = False\n",
    "\n",
    "print(\"Found best case validation loss to be \" + str(bestValLoss) + \"\\n... Loading saved model and testing\")\n",
    "debias_posModel.load_state_dict(torch.load(\"models/savedModels/debiased_model.m\"))\n",
    "test_loss = test(debias_posModel, device, formattedBatchedTestData, criterion)\n",
    "precision, recall, f1 = precisionRecallEval(debias_posModel, device, formattedBatchedTestData)\n",
    "print(\"TEST DATA -\\n\\tTest Loss: {:.6f} \\n\\tTest Precision {:6f} \\n\\tTest Recall {:6f} \\n\\tTest Macro F1 {:6f}\".format(test_loss, precision, recall, f1))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= BIASED MODEL TEST RESULTS =============\n",
      "loss: 0.00017368417236148794\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "============= DEBIASED MODEL TEST RESULTS =============\n",
      "loss: 0.00033492411393746196\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "============= MODEL COMPARISION RESULTS =============\n",
      "delta loss: 0.00016123994157597401\n",
      "delta precision: 0.0\n",
      "delta recall: 0.0\n",
      "delta f1: 0.0\n"
     ]
    }
   ],
   "source": [
    "posModel.load_state_dict(torch.load(\"models/savedModels/model.m\"))\n",
    "a_biased_precision, a_biased_recall, a_biased_f1 = precisionRecallEval(posModel, device, formattedBatchedBiasTestData)\n",
    "a_test_biased_loss = test(posModel, device, formattedBatchedBiasTestData, criterion)\n",
    "print(\"============= BIASED MODEL TEST RESULTS =============\")\n",
    "print(\"loss: \" + str(a_test_biased_loss))\n",
    "print(\"precision: \" + str(a_biased_precision))\n",
    "print(\"recall: \" + str(a_biased_recall))\n",
    "print(\"f1: \" + str(a_biased_f1))\n",
    "\n",
    "debias_posModel.load_state_dict(torch.load(\"models/savedModels/debiased_model.m\"))\n",
    "a_debiased_precision, a_debiased_recall, a_debiased_f1 = precisionRecallEval(debias_posModel, device, formattedBatchedBiasTestData)\n",
    "a_test_debiased_loss = test(debias_posModel, device, formattedBatchedBiasTestData, criterion)\n",
    "print(\"============= DEBIASED MODEL TEST RESULTS =============\")\n",
    "print(\"loss: \" + str(a_test_debiased_loss))\n",
    "print(\"precision: \" + str(a_debiased_precision))\n",
    "print(\"recall: \" + str(a_debiased_recall))\n",
    "print(\"f1: \" + str(a_debiased_f1))\n",
    "\n",
    "print(\"============= MODEL COMPARISION RESULTS =============\")\n",
    "print(\"delta loss: \" + str(a_test_debiased_loss - a_test_biased_loss))\n",
    "print(\"delta precision: \" + str(a_debiased_precision - a_biased_precision))\n",
    "print(\"delta recall: \" + str(a_debiased_recall - a_biased_recall))\n",
    "print(\"delta f1: \" + str(a_debiased_f1 - a_biased_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
